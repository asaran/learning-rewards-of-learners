{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2018-11-15-21-58-45-340634\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gym\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import torch\n",
    "from run_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the code for running trajectories with pong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try it just for two trajectories\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env_type = \"atari\"\n",
    "\n",
    "#env id, env type, num envs, and seed\n",
    "env = make_vec_env(env_id, 'atari', 1, 0,\n",
    "                   wrapper_kwargs={\n",
    "                       'clip_rewards':False,\n",
    "                       'episode_life':False,\n",
    "                   })\n",
    "\n",
    "\n",
    "env = VecFrameStack(env, 4)\n",
    "agent = PPO2Agent(env, env_type)\n",
    "#agent = RandomAgent(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_state(obs):\n",
    "    obs_highs = env.observation_space.high\n",
    "    obs_lows = env.observation_space.low\n",
    "    #print(obs_highs)\n",
    "    #print(obs_lows)\n",
    "    return  2.0 * (obs - obs_lows) / (obs_highs - obs_lows) - 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865 [-20.]\n",
      "1001 [-20.]\n",
      "1716 [-18.]\n",
      "2529 [-11.]\n",
      "2730 [21.]\n",
      "[array([-20.], dtype=float32), array([-20.], dtype=float32), array([-18.], dtype=float32), array([-11.], dtype=float32), array([21.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['00100','00200','00300','00400','03600']\n",
    "\n",
    "demonstrations = []\n",
    "learning_returns = []\n",
    "for checkpoint in checkpoints:\n",
    "    \n",
    "    model_path = \"./models/pong/checkpoints/\" + checkpoint\n",
    "    \n",
    "    agent.load_policy(model_path, env)\n",
    "    episode_count = 1\n",
    "    done = False\n",
    "    traj = []\n",
    "    r = 0\n",
    "    for i in range(episode_count):\n",
    "        ob = env.reset()\n",
    "        #traj.append(ob)\n",
    "        #print(ob.shape)\n",
    "        steps = 0\n",
    "        acc_reward = 0\n",
    "        while True:\n",
    "            action = agent.act(ob, r, done)\n",
    "            ob, r, done, _ = env.step(action)\n",
    "            #print(ob.shape)\n",
    "            traj.append(normalize_state(ob))\n",
    "            steps += 1\n",
    "            acc_reward += r\n",
    "            if done:\n",
    "                print(steps,acc_reward)\n",
    "                break\n",
    "                \n",
    "    demonstrations.append(traj)\n",
    "    learning_returns.append(acc_reward)\n",
    "    \n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "print(learning_returns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions for getting argmax action from TileCoding value function and normalizing observations to range [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to build a neural network to predict the reward the learner is trying to optimize. The inputs are 84x84x4 grayscale images. I'm going to try and use the NIPS architecture from DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2)\n",
    "        self.fc1 = nn.Linear(2592, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        \n",
    "    def cum_return(self, traj):\n",
    "        '''calculate cumulative return of trajectory'''\n",
    "        sum_rewards = 0\n",
    "        for x in traj:\n",
    "            x = x.permute(0,3,1,2)\n",
    "            #x = x.view(-1,4,84,84)\n",
    "            #compute forward pass of reward network\n",
    "            x = F.leaky_relu(self.conv1(x))\n",
    "            x = F.leaky_relu(self.conv2(x))\n",
    "            x = x.view(-1, 2592)\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            r = torch.tanh(self.fc2(x))\n",
    "            sum_rewards += r\n",
    "        ##    y = self.scalar(torch.ones(1))\n",
    "        ##    sum_rewards += y\n",
    "        #print(sum_rewards)\n",
    "        return sum_rewards\n",
    "        \n",
    "            \n",
    "    \n",
    "    def forward(self, traj_i, traj_j):\n",
    "        '''compute cumulative return for each trajectory and return logits'''\n",
    "        #print([self.cum_return(traj_i), self.cum_return(traj_j)])\n",
    "        return torch.cat([self.cum_return(traj_i), self.cum_return(traj_j)])\n",
    "    \n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 8, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 4, stride=2)\n",
    "        self.fc1 = nn.Linear(9*9*16, 1)\n",
    "        #self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        \n",
    "    def cum_return(self, traj):\n",
    "        '''calculate cumulative return of trajectory'''\n",
    "        sum_rewards = 0\n",
    "        for x in traj:\n",
    "            x = x.permute(0,3,1,2)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = x.view(-1, 9*9*16)\n",
    "            r = torch.tanh(self.fc1(x))\n",
    "            \n",
    "            sum_rewards += r\n",
    "        ##    y = self.scalar(torch.ones(1))\n",
    "        ##    sum_rewards += y\n",
    "        #print(sum_rewards)\n",
    "        return sum_rewards\n",
    "        \n",
    "            \n",
    "    \n",
    "    def forward(self, traj_i, traj_j):\n",
    "        '''compute cumulative return for each trajectory and return logits'''\n",
    "        #print([self.cum_return(traj_i), self.cum_return(traj_j)])\n",
    "        return torch.cat([self.cum_return(traj_i), self.cum_return(traj_j)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network. I'm just going to do it one by one for now. Could adapt it for minibatches to get better gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reward(reward_network, optimizer, trajectories, num_iter):\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    #print(training_data[0])\n",
    "    cum_loss = 0.0\n",
    "    for epoch in range(num_iter):\n",
    "        #pick two random trajectories, traj_i and traj_j such that i > j\n",
    "        j = np.random.randint(len(trajectories)-1) #make sure there is at least one later trajectory\n",
    "        i = np.random.randint(j+1,len(trajectories))\n",
    "        #print(i,j)\n",
    "        #traj_i = np.array([[d[0]] for d in trajectories[i]])\n",
    "        #traj_j = np.array([[d[0]] for d in trajectories[j]])\n",
    "        traj_i = np.array(trajectories[i])\n",
    "        traj_j = np.array(trajectories[j])\n",
    "        labels = np.array([[0]])\n",
    "        traj_i = torch.from_numpy(traj_i).float().to(device)\n",
    "        traj_j = torch.from_numpy(traj_j).float().to(device)\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        #zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward + backward + optimize\n",
    "        outputs = reward_network.forward(traj_i, traj_j).unsqueeze(0)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        loss = loss_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print stats to see if learning\n",
    "        item_loss = loss.item()\n",
    "        cum_loss += item_loss\n",
    "        if epoch % 20 == 19:\n",
    "            #with torch.no_grad():\n",
    "            #    print(torch.cat([reward_network.cum_return(torch.from_numpy(np.array(traj)).float()) for traj in trajectories]))\n",
    "            print(epoch, cum_loss / 100)\n",
    "            cum_loss = 0.0\n",
    "    print(\"finished training\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a reward network and optimize it using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = Net()\n",
    "reward.to(device)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(reward.parameters(), lr = 0.0001)\n",
    "learn_reward(reward, optimizer, demonstrations, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out learned return for all demos. should be roughly increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.cat([reward.cum_return(torch.from_numpy(np.array(traj)).float().to(device)) for traj in demonstrations]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at predicted reward over last demo. It's all +1 since demos are monotonically increasing. Maybe need to truncate demos to fixed length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#for last demo\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for d in demonstrations:\n",
    "        rewards = []\n",
    "        print(cnt)\n",
    "        for s in d:\n",
    "            r = reward.cum_return(torch.from_numpy(np.array([s])).float().to(device)).item()\n",
    "            rewards.append(r)\n",
    "        plt.figure(cnt)\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel(\"time\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        plt.title(\"true return = {}\".format(learning_returns[cnt]))\n",
    "        cnt += 1\n",
    "#plt.savefig(\"learned_mcar_return.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fix at length of first demo... Doesn't seem to really work. Maybe use smaller neural net? there's gotta be something different in the different demos, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = len(demonstrations[0])\n",
    "demos_fh = [d[len(d)-H:] for d in demonstrations]\n",
    "reward_fh = Net()\n",
    "reward_fh.to(device)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(reward_fh.parameters(), lr = 0.0001)\n",
    "learn_reward(reward_fh, optimizer, demos_fh, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.cat([reward_fh.cum_return(torch.from_numpy(np.array(traj)).float().to(device)) for traj in demos_fh]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in demos_fh:\n",
    "    print(len(d))\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for d in demos_fh:\n",
    "        rewards = []\n",
    "        print(cnt)\n",
    "        for s in d:\n",
    "            r = reward_fh.cum_return(torch.from_numpy(np.array([s])).float().to(device)).item()\n",
    "            rewards.append(r)\n",
    "        plt.figure(cnt)\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel(\"time\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        plt.title(\"true return = {}\".format(learning_returns[cnt]))\n",
    "        cnt += 1\n",
    "#plt.savefig(\"learned_mcar_return.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So now we want to optimize a policy using the learned reward to see how well it can perform if we run RL to convergence on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that makes a reward function out of a neural network\n",
    "class NNetReward:\n",
    "    def __init__(self,nnet):\n",
    "        self.nnet = nnet\n",
    "    def get_reward(self, state):\n",
    "        #transform to tensor and input to nnet\n",
    "        return self.nnet.cum_return(torch.from_numpy(np.array([state])).float()).item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SARSA tilecoding on learned reward function\n",
    "num_episodes = 1500\n",
    "\n",
    "numOfTilings = 8\n",
    "alpha = 0.5\n",
    "n = 1\n",
    "\n",
    "nn_reward = NNetReward(reward)\n",
    "\n",
    "\n",
    "# use optimistic initial value, so it's ok to set epsilon to 0\n",
    "EPSILON = 0.0\n",
    "discount = 0.999 #using high discount factor\n",
    "\n",
    "apprenticeVFunction = ValueFunction(alpha, numOfTilings)\n",
    "for i in range(num_episodes):\n",
    "    if i % 100 == 99:\n",
    "        print(i, steps)\n",
    "    r, s, steps = run_episode(env, apprenticeVFunction, n, False, EPSILON, max_time = 2000, reward_fn = nn_reward)\n",
    "    \n",
    "print(\"done training\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the learned policy\n",
    "returns = evaluate_policy(env, 200, apprenticeVFunction)\n",
    "print(\"best, worst, average\", np.max(returns), np.min(returns), np.mean(returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluate demonstrations\")\n",
    "demo_returns = learning_returns\n",
    "\n",
    "print(\"best worst, average\", np.max(demo_returns), np.min(demo_returns), np.mean(demo_returns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
